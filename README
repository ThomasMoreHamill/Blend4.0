
#!/bin/tcsh

# ============================================================
# point of contact: Tom Hamill, tom.hamill@noaa.gov,
# +1 (303) 497-3060.
#
# This documentation is for a (mostly) new way of achieving well-
# calibrated, statistically postprocessed forecasts of
# probabilistic precipitation amount based on multi-model
# ensemble guidance, trained against gridded precipitation
# analyses.
#
# To understand this algorithm you should read Hamill et al.
# (2017; https://doi.org/10.1175/MWR-D-16-0331.1) and the 
# recently submitted article 
# Hamill, T. M., and Scheuerer, M., 2018: Probabilistic precipitation 
# forecast postprocessing using quantile mapping and rank-weighted 
# best-member dressing.  Mon. Wea. Rev., submitted.   Also: 
# Online appendix 1.  Available at
# https://www.esrl.noaa.gov/psd/people/tom.hamill/publications.html
#
# This skeleton script is intended to show NOAA partners the
# components I have put together to generate improved statistically
# post-processed probabilistic forecasts of precipitation.
# Several big changes for this version relative to the
# previous one documented above include:
#
# (a) a new way of generating the underlying CDFs used for
# quantile mapping that should dramatically save storage space,
# making it easier to incorporate new models without saving
# massive amounts of data on disk.  CDFs are estimated with
# three parameters, a fraction of samples with zero precipitation
# and a classical Gamma distribution with shape and scale
# parameters.   Distributions vary with forecast lead time,
# physical location, and are based on the last 60 days of
# precipitation forecasts and analyses, with sample size
# enlarged via "supplemental locations."
#
# (b) the early processing of the forecasts model-by-model, as
# opposed to all the forecast data being combined.  Subsequent
# to the step of processing each model individually, there
# is a new step to linearly combine probabilities from the
# constituent systems to form the final multi-model ensemble
# forecast.
#
# (c) removal of Savitzky-Golay spatial smoothing of output
# probability fields used in the previous version, based on
# user feedback requesting more spatial detail in the
# mountainous western US.
#
# (d) use of a 5x5 stencil of surrounding grid points instead
# of the previous 3x3 stencil when quantile mapping, both to
# ameliorate sampling error, and partially obviate the need
# for the Savitzky-Golay smoothing.
#
# (e) implicitly, the generation of a full PQPF.  The user can
# revise the code to store out exceedance probability forecasts
# for any desired precipitation amount.   Currently the code
# stores 0.254 (POP), 1.0, 2.5, 5.0, 10.0, and 25.0 mm threshold
# probabilities.
#
# The code that is described here is not shrink-wrapped and
# ready for use with the anticipated higher-resolution 2.5-km
# data desired at NOAA MDL with next-generation products.  Given
# that adaptation will be needed anyway, I instead aimed to go
# through the sequence of processing I performed to generate
# forecasts at the current 1/8-degree grid spacing.  MDL, or
# other users hopefully can take the components and adapt
# them to the locations and grids of their choosing.
#
# My hope is that it with this documentation and associated
# Powerpoints and journal articles, is easy enough to see the 
# functionalities of the new software, and it can be integrated 
# with other improvements such as the use of higher-resolution 
# training data.
#
# Comment for NOAA MDL only:
# -------------------------
# One complication relative to operations is that for my own
# convenience, but to MDL's detriment, I moved my data processing
# back to the smaller 464x224 grid of the CCPA precipitation
# analyses, barely covering the CONUS.   MDL staff will
# have to back that out as you adapt code.
# Presumably this isn't as much of an issue as you have to
# adapt code for the higher-resolution 2.5-degree data.
#
# Questions?  Tom Hamill, tom.hamill@noaa.gov, (303) 497-3060
#
# ============================================================
#
# ---- Before any processing of the forecast data is performed,
#      "supplemental locations" need to be pre-determined.   When
#      statistically postprocessing any particular (i,j) grid
#      point, the supplemental locations for (i,j) are a list of
#      other (i', j') locations that are used to supplement
#      the training data at (i,j).  Per the journal article
#      cited in the documentation above, the supplemental
#      locations are based upon the similarity of precipitation
#      climatology, terrain height, terrain orientation, and
#      physical distance.
#
#      Supplemental locations on the 1/8-degree grid
#      for a given month 01-12 were computed with

compute_precip_analog_locations_ccpa9.x mm

#      where mm is the month. 
#
# ---- Note that I started this lastest PQPF development
#      process by copying over an earlier version
#      of National Blend software from NOAA's theia
#      supercomputer and then modifying it so that I could add
#      new aspects such as including ECMWF data.  A preliminary
#      step was performed where I downloaded NCEP, ECMWF, and
#      CMC ensemble data from the ECMWF TIGGE forecast
#      web site, where I believe the data was first saved at
#      1/2-degree grid spacing around the CONUS.   According
#      to a personal conversation with Roberto Buizza
#      of ECMWF, it's likely that the procedure there
#      for providing data on a grid of choice is to
#      just take the nearest grid point's value on the
#      model's native grid.   This probably introduced
#      some inaccuracy relative to what MDL may have
#      with its interpoation procedures.
#
#      TIGGE data was downloaded in grib format.   I provide no
#      scripts or code for this download part of my processing,
#      as what other potential partners will do is different.

# ---- The input grib files were then split up, so
#      that the data was separated by system (CMC, NCEP,
#      ECMWF ensembles), by lead time, and by initial
#      condition date and hour.   This was achieved
#      with the python script

python gribfile_split.py infilename

#      where infilename is the input grib file name to
#      split up.  Data was output in grib files.

# ---- The script below moves the data from grib files to
#      netCDF files at 1/8-degree spacing over the CONUS for
#      a range of dates specified in the script.   Say I was
#      interested in archiving in netCDF the sample forecasts
#      for 12 and 24 h for the ECMWF system, which I'd need
#      to calculate the accumulated precipitation in the 12-24
#      h period.  I'd generate such files with the following:

python precip_forecast_ccpa_2netcdf.py ECMWF 12
python precip_forecast_ccpa_2netcdf.py ECMWF 24

# ---- In this version of the code, we are simplifying the quantile
#      mapping drastically in order to save disk space (lots!).
#      The previous postprocessing saved out forecast and analyzed CDFs at
#      lots of pre-defined precipitation amounts.   The new algorithm
#      will estimate the forecast and analyzed distributions with
#      three parameters, a fraction zero (the fraction of the time
#      where the forecast or analyzed precipitation is zero) and then
#      for positive amounts, a Gamma shape and scale parameter.
#
#      To understand the new procedure for quantile mapping 
#      it helps to look at the Wilks "Statistical Methods
#      in the Atmospheric Sciences" textbook.   Look up the section on
#      Gamma distributions (in chapter 4, in my version, 3rd Ed.).
#      Gamma distributions are valid only for positive numbers,
#      and precipitation can obviously be zero.   Hence, we aim to
#      save the information on this day necessary at a later point to
#      estimate the precipitation CDF with three parameters: (1) the
#      fraction of samples with zero precipitation, and (2) for positive
#      precipitation, the Gamma shape parameter (alpha), and (3) for
#      positive precipitation, the Gamma scale parameter (beta). In
#      the Wilks text, there is no mention of "fraction zero."
#      This we will estimate simply, by just keeping track, grid
#      point by grid point, of the number of points with zero and
#      nonzero precipitation.  Later, we'll accumulate information
#      over many days and estimate the fraction zero from relative
#      frequency.   The Gamma shape and scale parameters are more
#      complicated.  There is a section in the Wilks text on using
#      the D statistic, D = ln(xbar) - (1/n)sum(ln(xi)).  What we
#      are going to do to simplify the calculations is to save out
#      the information needed to calculate D at a later point, with
#      data summed over many case days and including supplemental
#      location's data.  We'll need to save information to calculate
#      at a later point the mean precipitation (xbar) and the sum of
#      logarithm of the members' precipitation amounts (ln(xi)).
#      This program is run out of the compute_singleday_gamma_stats
#      directory, like 

python compute_singleday_gamma_stats.py 2016010100 ECMWF 24

#      where 2016010100 is the date, ECMWF is the model, and 24 is the 
#      forecast lead time.   There is a script 
#      compute_singleday_gamma_stats.deck which can be run to 
#      execute this many times; this script was created by 
#      compute_singleday_gamma_stats.py

# ---- Gamma distributions are now estimated for the
#      forecast and analyzed precipitation amounts for quantile
#      mapping.  With the CDFs estimated, we perform a quantile 
#      mapping and then tally up the closest-member histogram
#      statistics that are used in an objective re-weighting of the
#      sorted members (in a later step).   The CDF parameter estimation,
#      quantile mapping, and saving of the resultant closest-member
#      histogram statistics is accomplished with
#      something like

generate_dressing_stats_anymodel_gammacdf.x  2016033100 ECMWF 24

#      Data above is written to a netcdf file.  This file contains
#      intermediate information like the closest-histogram statistics
#      for a given day, and (see notes above) the intermediate
#      information necessary to calculate the D statistics for dressing.
#      Note that this same program above will have to be run many times
#      over.   When we actually get to the processing, say, of
#      2016040100 data, presumably the program above will have been
#      already run for each of the preceding 60 days.  The good thing
#      in a production environment, though, is that as you proceed on to
#      the next day, 2016040200, the only thing you'll have to do
#      is to generate error statistics now for 2016040100; all the
#      previous dates will already have been generated and presumably
#      are sitting there on disk waiting to be used.  This will
#      speed up the data processing considerably.

#      Scripts that run generate_dressing_stats_anymodel_gammacdf.x 
#      over many leads, dates, and models are in the same 
#      generate_dressing_stats_anymodel_gammacdf folder.

# ---- Let's assume now that generate_dressing_stats_anymodel_gammacdf.x
#      has been run for each of the previous 60 days.  Before we
#      generate a post-processed forecast, we're going to synthesize
#      that information from the previous 60 days.   This is achieved
#      by the python script dressing_statistics_to_netcdf.py run from
#      the dressing_statistics_to_netcdf folder.  Below, you enter the 
#      model name and ending lead time of the forecast and date for 
#      which you want the statistics tallied.  The output are two
#      netCDF files with dressing statistics that are read in by the
#      final program that generates the forecasts.

python dressing_statistics_to_netcdf.py ECMWF 24 2016040100

#      This will save the resulting output to a netCDF file with 
#      a name like outfile_nc = data_directory+cmodel+'/closest_histogram_'+
#      cmodel+'_date='+date_forecast+'_lead='+cleade+'.nc'

# ---- now we actually generate forecasts, quantile mapping
#      a given member and a stencil of the 5 x 5 surrounding grid points,
#      re-weighting it with the closest-member histograms,
#      and dressing it with Gaussian-distributed noise. This is 
#      accomplished by going to the blend_precip_singlemodel_dressed_gammacdf
#      folder and running the program

blend_precip_singlemodel_dressed_gammacdf.x 2016040100 24 ECMWF

#      It will saves out the forecast probabilities to a netCDF file.

# ---- The final step in the forecast process would be forming 
#      the weighted linear combination of post-processed
#      probabilistic forecasts.   In my software below, since
#      I am performing this after the fact, I combine this step
#      with the forecast verification.   To blend and verify the
#      forecasts, go to the plot_reliability_mme folder.
#
#      compute_bss.py is used to combine the multi-model guidance
#      and compute Brier skill scores, dumping them out to a file.
#      These scores are then displayed with plot_bss.py.  
#      Reliability scores are calculated and plotted with 
#      the script plot_reliability_mme.py, which loops over
#      lead times and precipitation thresholds

python plot_reliability_mme.py 

#
# ---- Plotting scripts may be of interest.   To plot a
#      single model's forecast at various stages of
#      postprocessing, go to the display_prob_forecasts_singlemodel
#      folder and run display_prob_forecasts_singlemodel.py.
#      For example, to display forecasts for 1 April 2016,
#      12-24 h lead, and the POP amount (0.254 mm), do the
#      following:

python display_prob_forecasts_mme.py 2016040100 24 POP
